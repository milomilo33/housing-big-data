version: '2' 

services:

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./primary-dataset:/home/housing-big-data/primary-dataset
    environment:
      - CLUSTER_NAME=housing-big-data
    env_file:
      - ./docker-spark/hadoop.env
    ports:
      - 9870:9870
      - 9000:9000
    # command: /bin/sh -c "/home/housing-big-data/primary-dataset/initialize.sh"
    # command: /home/housing-big-data/primary-dataset/initialize.sh

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    depends_on: 
      - namenode
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    env_file:
      - ./docker-spark/hadoop.env

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    depends_on: 
      - namenode
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    env_file:
      - ./docker-spark/hadoop.env

  spark-master:
    image: bde2020/spark-master:3.0.1-hadoop3.2
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
    environment:
      - PYSPARK_PYTHON=python3
      - MONGO_URI=mongodb://root:example@mongodb-housing:27017/
      # - SPARK_CLASSPATH=/spark/jars/mongo-spark-connector.jar
    env_file:
      - ./docker-spark/hadoop.env
    volumes: 
      - ./batch-analysis:/home/housing-big-data/batch-analysis
      - ./real-time-analysis:/home/housing-big-data/real-time-analysis
      # - ./docker-spark/mongo-spark-connector.jar:/spark/jars/mongo-spark-connector.jar
    command: /bin/bash -c "python3 -m pip install pymongo && tail -f /dev/null"
    # tty: true
    # stdin_open: true

  spark-worker1:
    image: bde2020/spark-worker:3.0.1-hadoop3.2
    container_name: spark-worker1
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - 8081:8081
    env_file:
      - ./docker-spark/hadoop.env
  
  spark-worker2:
    image: bde2020/spark-worker:3.0.1-hadoop3.2
    container_name: spark-worker2
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - 8082:8081
    env_file:
      - ./docker-spark/hadoop.env
  
  hue:
    image: gethue/hue:20201111-135001
    hostname: hue
    container_name: hue
    dns: 8.8.8.8
    ports:
      - "8888:8888"
    volumes:
      - ./docker-spark/conf.dist:/usr/share/hue/desktop/conf
    depends_on: 
      - namenode

  mongodb-housing:
    image: mongo
    container_name: mongodb-housing
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
      MONGO_INITDB_DATABASE: housing_data
    volumes:
        - ./mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 5s

  metabase:
    image: metabase/metabase
    container_name: metabase
    ports:
      - "3000:3000"
    # environment:
    #   - MB_DB_TYPE=mongo
    #   - MB_DB_DBNAME=housing_data
    #   - MB_DB_PORT=27017
    #   - MB_DB_USER=root
    #   - MB_DB_PASS=example
    #   - MB_DB_HOST=mongodb-housing
    depends_on:
      mongodb-housing:
        condition: service_healthy

  zookeeper:
    image: bitnami/zookeeper:3.6.4
    container_name: zookeeper
    ports:
      - 2181:2181
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes

  kafka:
    image: bitnami/kafka:3.1.2
    container_name: kafka
    ports:
      - 19092:19092
      - 9092:9092
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:19092
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:19092
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=1
    depends_on:
      - zookeeper

  crime-data-producer:
    build:
      args:
        PYTHON_PRODUCER_VERSION: 3.7-slim
      context: ./crime-data-producer
    container_name: crime-data-producer
    volumes:
      - ./crime-data-producer:/app:ro
    environment:
      KAFKA_BROKER: kafka:9092
      KAFKA_TOPIC: crime-data
    depends_on:
      - kafka

volumes:
  hadoop_namenode:
  hadoop_datanode1:
  hadoop_datanode2:
