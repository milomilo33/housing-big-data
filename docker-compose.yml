version: '2' 

services:

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./primary-dataset:/home/housing-big-data/primary-dataset
    environment:
      - CLUSTER_NAME=housing-big-data
    env_file:
      - ./docker-spark/hadoop.env
    ports:
      - 9870:9870
      - 9000:9000
    # command: /bin/sh -c "/home/housing-big-data/primary-dataset/initialize.sh"
    # command: /home/housing-big-data/primary-dataset/initialize.sh

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    depends_on: 
      - namenode
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    env_file:
      - ./docker-spark/hadoop.env

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    depends_on: 
      - namenode
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    env_file:
      - ./docker-spark/hadoop.env

  spark-master:
    image: bde2020/spark-master:3.0.1-hadoop3.2
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
    environment:
      - PYSPARK_PYTHON=python3
      - MONGO_URI=mongodb://root:example@mongodb-housing:27017/
      # - SPARK_CLASSPATH=/spark/jars/mongo-spark-connector.jar
    env_file:
      - ./docker-spark/hadoop.env
    volumes: 
      - ./batch-analysis:/home/housing-big-data/batch-analysis
      - ./real-time-analysis:/home/housing-big-data/real-time-analysis
      # - ./docker-spark/mongo-spark-connector.jar:/spark/jars/mongo-spark-connector.jar
    command: /bin/bash -c "python3 -m pip install pymongo && tail -f /dev/null"
    # tty: true
    # stdin_open: true

  spark-worker1:
    image: bde2020/spark-worker:3.0.1-hadoop3.2
    container_name: spark-worker1
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - 8081:8081
    env_file:
      - ./docker-spark/hadoop.env
  
  spark-worker2:
    image: bde2020/spark-worker:3.0.1-hadoop3.2
    container_name: spark-worker2
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - 8082:8081
    env_file:
      - ./docker-spark/hadoop.env
  
  hue:
    image: gethue/hue:20201111-135001
    hostname: hue
    container_name: hue
    dns: 8.8.8.8
    ports:
      - "8888:8888"
    volumes:
      - ./docker-spark/conf.dist:/usr/share/hue/desktop/conf
    depends_on: 
      - namenode

  mongodb-housing:
    image: mongo
    container_name: mongodb-housing
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
      MONGO_INITDB_DATABASE: housing_data
    volumes:
        - ./mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 5s

  metabase:
    image: metabase/metabase
    container_name: metabase
    ports:
      - "3000:3000"
    # environment:
    #   - MB_DB_TYPE=mongo
    #   - MB_DB_DBNAME=housing_data
    #   - MB_DB_PORT=27017
    #   - MB_DB_USER=root
    #   - MB_DB_PASS=example
    #   - MB_DB_HOST=mongodb-housing
    depends_on:
      mongodb-housing:
        condition: service_healthy

volumes:
  hadoop_namenode:
  hadoop_datanode1:
  hadoop_datanode2:
